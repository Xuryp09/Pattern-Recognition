{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7Tb9hStqn0am"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "def load_pkl(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return np.array(data)\n",
        "\n",
        "def get_train(data_path):\n",
        "    files = []\n",
        "    labels = []\n",
        "    for dir in ['class_0', 'class_1']:\n",
        "        class_path = os.path.join(data_path, dir)\n",
        "        for bag in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, bag)\n",
        "            files.append(file_path)\n",
        "            labels.append(1 if dir == 'class_1' else 0)\n",
        "    return files, labels\n",
        "\n",
        "def get_test(data_path):\n",
        "    files = []\n",
        "    for bag in os.listdir(data_path):\n",
        "        file_path = os.path.join(data_path, bag)\n",
        "        files.append(file_path)\n",
        "    return files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the train_path and test_path to the correct directory."
      ],
      "metadata": {
        "id": "UFSO20my0JOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/drive/MyDrive/Colab Notebooks/train'\n",
        "test_path = '/content/drive/MyDrive/Colab Notebooks/test'\n",
        "train_files, train_labels = get_train(train_path)\n",
        "print(len(train_files))"
      ],
      "metadata": {
        "id": "pMoRJC7Pt3IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_files = get_test(test_path)\n",
        "print(len(test_files))"
      ],
      "metadata": {
        "id": "Og4p3GNSt42W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalMaxPooling1D, Lambda"
      ],
      "metadata": {
        "id": "T8Z_aU6Tt6fV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(img):\n",
        "    img = img_to_array(img)\n",
        "    img = tf.image.resize(img, (128, 128))\n",
        "    img = preprocess_input(img)\n",
        "    return img"
      ],
      "metadata": {
        "id": "RrQJwP14t8E_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base = ResNet50(weights='imagenet', include_top=False, pooling='avg')"
      ],
      "metadata": {
        "id": "c-R85VzQ2enq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(img):\n",
        "    features = []\n",
        "    for i in img:\n",
        "        features.append(preprocess_image(i))\n",
        "    processed_images = np.array(features)\n",
        "    features = base.predict(processed_images)\n",
        "    return features"
      ],
      "metadata": {
        "id": "CoqgYPYJ3d_x"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MIL_model(original_input):\n",
        "    inputs = Input(shape=original_input)\n",
        "    x = GlobalMaxPooling1D()(inputs)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "0E0xFsTnt98D"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MIL_model((256, 2048))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "9OAogCYm5Dzo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_data(files, labels, batch_size=8):\n",
        "    while True:\n",
        "        indices = np.arange(len(files))\n",
        "        np.random.shuffle(indices)\n",
        "        for start_idx in range(0, len(files), batch_size):\n",
        "            batch_indices = indices[start_idx:start_idx + batch_size]\n",
        "            batch_files = []\n",
        "            batch_labels = []\n",
        "            batch_images = []\n",
        "            batch_features = []\n",
        "            for i in batch_indices:\n",
        "              batch_files.append(files[i])\n",
        "              batch_labels.append(labels[i])\n",
        "\n",
        "            for i in batch_files:\n",
        "              batch_images.append(load_pkl(i))\n",
        "\n",
        "            for img in batch_images:\n",
        "              batch_features.append(extract_features(img))\n",
        "\n",
        "            batch_features = np.array(batch_features)\n",
        "            batch_labels = np.array(batch_labels)\n",
        "            yield batch_features, batch_labels\n",
        "\n",
        "batch_size = 8\n",
        "batch_train = batch_data(train_files, train_labels, batch_size=batch_size)\n",
        "step = len(train_files) // batch_size\n",
        "model.fit(batch_train, steps_per_epoch=step, epochs=10)"
      ],
      "metadata": {
        "id": "u-cPmujSuByd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_test_data(files, batch_size=8):\n",
        "    while True:\n",
        "        indices = np.arange(len(files))\n",
        "        np.random.shuffle(indices)\n",
        "        for start_idx in range(0, len(files), batch_size):\n",
        "            batch_indices = indices[start_idx:start_idx + batch_size]\n",
        "            batch_files = []\n",
        "            batch_images = []\n",
        "            batch_features = []\n",
        "            for i in batch_indices:\n",
        "              batch_files.append(files[i])\n",
        "\n",
        "            for i in batch_files:\n",
        "              batch_images.append(load_pkl(i))\n",
        "\n",
        "            for img in batch_images:\n",
        "              batch_features.append(extract_features(img))\n",
        "\n",
        "            batch_features = np.array(batch_features)\n",
        "            yield batch_features"
      ],
      "metadata": {
        "id": "bbybvohx9wZx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_test = batch_test_data(test_files, batch_size=batch_size)\n",
        "steps = len(test_files) // batch_size\n",
        "predictions = model.predict(batch_test, steps=steps)\n",
        "predictions = (predictions > 0.5).astype(int).flatten()"
      ],
      "metadata": {
        "id": "jMXwCl6J_EoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/test'\n",
        "files = os.listdir(folder_path)\n",
        "pkl_filenames = []\n",
        "for pkl_file in files:\n",
        "    filename = os.path.splitext(pkl_file)[0]\n",
        "    pkl_filenames.append(filename)"
      ],
      "metadata": {
        "id": "J_PTAz6s_ZCU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "output_csv = \"output.csv\"\n",
        "with open(output_csv, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['image_id', 'y_pred'])\n",
        "    for i in range(len(pkl_filenames)):\n",
        "        writer.writerow([pkl_filenames[i], predictions[i]])"
      ],
      "metadata": {
        "id": "4bxtsYbK_xrI"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}